#!/usr/bin/env yog

class Token
  def init(lineno, value=nil)
    self.lineno = lineno
    self.value = value
  end
end

class Indent > Token
end

class Dedent > Token
end

class Klass > Token
end

class Base > Token
end

class Incluing > Token
end

class Method > Token
end

class Parameters > Token
end

class Return > Token
end

class Data > Token
end

class Attribute > Token
end

class Property > Token
end

class Classmethod > Token
end

class Function > Token
end

class Colon > Token
end

class Newline > Token
end

class Text > Token
end

KEY2TOKEN = {
  "class" => Klass,
  "base" => Base,
  "including" => Incluing,
  "method" => Method,
  "parameters" => Parameters,
  "return" => Return,
  "attribute" => Attribute,
  "classmethod" => Classmethod,
  "function" => Function,
  "property" => Property,
  "data" => Data }

class Lexer
  def init(fp)
    self.fp = fp
    self.lineno = 0
    self.margin_stack = [0]
    self.text_last? = false
    self.tokens = []
  end

  def generate_indent_tokens(line)
    margin = (line =~ /\A\s*/).end(0)
    if self.margin_stack[-1] < margin
      if !self.text_last?
        self.margin_stack.push(margin)
        self.tokens << Indent.new(self.lineno)
      end
      return
    end

    while !self.margin_stack.empty?
      if self.margin_stack[-1] <= margin
        return
      end
      self.margin_stack.pop()
      self.tokens << Dedent.new(self.lineno)
    end
  end

  def readline()
    comment_depth = 0
    while line = self.fp.readline()
      self.lineno += 1
      if line =~ /\A\s*--/
        comment_depth += 1
        next
      end
      if line =~ /\A\s*\+\+/
        comment_depth -= 1
        next
      end
      if 0 < comment_depth
        next
      end

      if line =~ /\A\s*#/
        next
      end
      if !self.text_last? && (line =~ /\A\s*\z/)
        next
      end
      break
    end
    return line
  end

  def split_key_value(line)
    size = line.size
    i = 0
    while i < size
      if line[i] == ":"
        i += 1
        if line[i] != ":"
          key = line.slice(0, i - 1).trim()
          value = line.slice(i).trim()
          return key, value
        end
      end
      i += 1
    end

    return nil, nil
  end

  def cut_margin(line)
    margin = self.margin_stack[-1]
    if line.size < margin
      return ""
    end
    return line.slice(margin)
  end

  def read_tokens()
    line = self.readline()
    if line == nil
      self.generate_indent_tokens("")
      return
    end
    self.generate_indent_tokens(line)

    key, value = self.split_key_value(line)
    if key != nil
      self.tokens << KEY2TOKEN.get(key, Text).new(self.lineno, key)
      self.tokens << Colon.new(self.lineno)
      self.tokens << Text.new(self.lineno, value)
      self.tokens << Newline.new(self.lineno)
      self.text_last? = false
      return
    end

    self.tokens << Text.new(self.lineno, self.cut_margin(line.gsub("::", ":")))
    self.text_last? = true
  end

  def unget_token(token)
    self.tokens.unshift(token)
  end

  def get_next_token()
    if self.tokens.empty?
      self.read_tokens()
      if self.tokens.empty?
        return nil
      end
    end
    return self.tokens.shift()
  end
end

class SyntaxError > Exception
end

class Node
end

class TextNode > Node
end

class ClassNode > Node
  def init()
    self.name = nil
    self.base = nil
    self.including = nil
    self.attrs = []
  end
end

class Parser
  def raise_syntax_error(lineno, msg)
    raise SyntaxError.new("line {0}: {1}".format(lineno, msg))
  end

  def skip_colon(lexer)
    token = lexer.get_next_token()
    if !token.kind_of?(Colon)
      self.raise_syntax_error(token.lineno, "token must be a colon")
    end
  end

  def skip_newline(lexer)
    token = lexer.get_next_token()
    if !token.kind_of?(Newline)
      self.raise_syntax_error(token.lineno, "token must be a newline")
    end
  end

  def get_text_token(lexer)
    token = lexer.get_next_token()
    if !token.kind_of?(Text)
      self.raise_syntax_error(token.lineno, "token must be a text")
    end
    return token
  end

  def parse_class(lexer)
    node = ClassNode.new()
    self.skip_colon(lexer)
    node.name = self.get_text_token(lexer).value
    self.skip_newline(lexer)

    token = lexer.get_next_token()
    if !token.kind_of?(Indent)
      lexer.unget_token(token)
      return node
    end

    while token = lexer.get_next_token()
      if token.kind_of?(Base)
        self.skip_colon(lexer)
        node.name = self.get_text_token(lexer).value
        self.skip_newline(lexer)
      elif token.kind_of?(Incluing)
        self.skip_colon(lexer)
        node.including = self.get_text_token(lexer).value
        self.skip_newline(lexer)
      elif token.kind_of?(Method)
        # TODO
        #node.attrs << self.parse_method(lexer)
      elif token.kind_of?(Property)
        # TODO
        #node.attrs << self.parse_property(lexer)
      elif token.kind_of?(Attribute)
        # TODO
        #node.attrs << self.parse_attribute(lexer)
      elif token.kind_of?(Dedent)
        return node
      else
        self.raise_syntax_error(token.lineno, "invalid token in class")
      end
    end
  end

  def parse_top_level(lexer)
    nodes = []
    while token = lexer.get_next_token()
      if token.kind_of?(Text)
        # TODO
      elif token.kind_of?(Klass)
        nodes << self.parse_class(lexer)
      elif token.kind_of?(Data)
        # TODO
      elif token.kind_of?(Function)
        # TODO
      else
        self.raise_syntax_error(token.lineno, "token must be a text, class, data or function")
      end
    end
    return nodes
  end

  def parse(fp)
    return self.parse_top_level(Lexer.new(fp))
  end
end

class HtmlGenerator
  def write(fp, nodes)
    # TODO
  end
end

nodes = nil
File.open(ARGV[1]) do [fp]
  nonlocal nodes
  nodes = Parser.new().parse(fp)
end
File.open(ARGV[2], "w") do [fp]
  HtmlGenerator.new().write(fp, nodes)
end

# vim: tabstop=2 shiftwidth=2 expandtab softtabstop=2 filetype=yog
